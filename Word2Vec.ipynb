{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3Ob_cPqV9l2e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- load your data ---\n",
        "df = pd.read_csv(\"C:/Users/Deepanshu/Downloads/grocery_transactions.csv\")  # change path\n",
        "df[\"product_id\"] = df[\"product_id\"].astype(str)\n",
        "\n",
        "# (optional) basic cleaning\n",
        "df = df.dropna(subset=[\"order_id\", \"product_id\"])\n",
        "\n",
        "# group by order → list of product_ids\n",
        "baskets = (\n",
        "    df.groupby(\"order_id\")[\"product_id\"]\n",
        "      .apply(list)\n",
        "      .tolist()\n",
        ")\n",
        "\n",
        "# Keep product metadata for reporting\n",
        "prod_meta = (\n",
        "    df.drop_duplicates(\"product_id\")\n",
        "      .set_index(\"product_id\")[[\"product_name\",\"category\"]]\n",
        "      .fillna({\"product_name\":\"\", \"category\":\"\"})\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NK_ERF1o-i7E"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "max_basket_size = max(len(b) for b in baskets)\n",
        "# key hyperparameters:\n",
        "VECTOR_SIZE = 100     # 50–200 is typical\n",
        "WINDOW = max_basket_size            # context window; 3–10 works well\n",
        "MIN_COUNT = 3         # drop very rare items; tune for your catalog size\n",
        "NEGATIVE = 10         # negative sampling\n",
        "EPOCHS = 100\n",
        "\n",
        "model = Word2Vec(\n",
        "    sentences=baskets,\n",
        "    vector_size=VECTOR_SIZE,\n",
        "    window=WINDOW,\n",
        "    min_count=MIN_COUNT,\n",
        "    sg=1,              # 1 = skip-gram (usually better); 0 = CBOW\n",
        "    negative=NEGATIVE,\n",
        "    workers=8,         # set to CPU cores\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# convenient references\n",
        "wv = model.wv\n",
        "vocab_items = list(wv.key_to_index.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HWkQdD7oJVUh"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_items = df[\"product_id\"].astype(str).unique().tolist()\n",
        "\n",
        "# Build counts on unique items per order to avoid double counting\n",
        "basket_sets = [set(b) for b in baskets]\n",
        "item_counts = Counter()\n",
        "pair_counts = Counter()\n",
        "for s in basket_sets:\n",
        "    items = list(s)\n",
        "    item_counts.update(items)\n",
        "    # count unordered pairs within the same basket\n",
        "    for i in range(len(items)):\n",
        "        for j in range(i+1, len(items)):\n",
        "            a, b = sorted((items[i], items[j]))\n",
        "            pair_counts[(a, b)] += 1\n",
        "\n",
        "def jaccard(pid, qid):\n",
        "    a, b = sorted((pid, qid))\n",
        "    co = pair_counts.get((a, b), 0)\n",
        "    if co == 0:\n",
        "        return 0.0\n",
        "    return co / (item_counts[pid] + item_counts[qid] - co)\n",
        "\n",
        "# 3) Embedding neighbors (cosine) if available; else fallback to Jaccard\n",
        "import numpy as np\n",
        "\n",
        "vocab_index = {k:i for i,k in enumerate(vocab_items)}\n",
        "emb = np.vstack([wv[k] for k in vocab_items])\n",
        "emb_norm = emb / (np.linalg.norm(emb, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "def topk_neighbors_embed(pid, k=10):\n",
        "    if pid not in vocab_index:\n",
        "        return []\n",
        "    i = vocab_index[pid]\n",
        "    sims = emb_norm @ emb_norm[i]\n",
        "    sims[i] = -1.0\n",
        "    idx = np.argpartition(-sims, k)[:k]\n",
        "    idx = idx[np.argsort(-sims[idx])]\n",
        "    return [(vocab_items[j], float(sims[j])) for j in idx]\n",
        "\n",
        "def topk_neighbors_fallback(pid, k=10):\n",
        "    # Rank by Jaccard co-occurrence\n",
        "    scores = []\n",
        "    for other in all_items:\n",
        "        if other == pid:\n",
        "            continue\n",
        "        s = jaccard(pid, other)\n",
        "        if s > 0:\n",
        "            scores.append((other, s))\n",
        "    scores.sort(key=lambda x: -x[1])\n",
        "    return scores[:k]\n",
        "\n",
        "# 4) Make per-product recommendations (guaranteed coverage)\n",
        "def category(pid):\n",
        "    return prod_meta.loc[pid, \"category\"] if pid in prod_meta.index else \"\"\n",
        "\n",
        "def nice_name(pid):\n",
        "    nm = prod_meta.loc[pid, \"product_name\"] if pid in prod_meta.index else \"\"\n",
        "    return nm if nm else pid\n",
        "\n",
        "def recommend_for_all(k_per_item=5, min_score=0.15, cross_category=True, prefer_embed=True):\n",
        "    rows = []\n",
        "    for pid in all_items:\n",
        "        # Try embeddings first\n",
        "        nbrs = topk_neighbors_embed(pid, k=k_per_item) if prefer_embed else []\n",
        "        # If none or too few, backfill with Jaccard\n",
        "\n",
        "        if len(nbrs) < k_per_item:\n",
        "            print(\"Jaccab\")\n",
        "            need = k_per_item - len(nbrs)\n",
        "            # Prevent overlap with existing neighbors\n",
        "            taken = set([n for n,_ in nbrs] + [pid])\n",
        "            fb = [(n,s) for n,s in topk_neighbors_fallback(pid, k=need*4) if n not in taken]\n",
        "            nbrs += fb[:need]\n",
        "\n",
        "        # Apply business filters\n",
        "        kept = []\n",
        "        for nid, score in nbrs:\n",
        "            if cross_category and category(pid) and category(pid) == category(nid):\n",
        "                continue\n",
        "            if score < min_score:\n",
        "                continue\n",
        "            kept.append((nid, score))\n",
        "            if len(kept) == k_per_item:\n",
        "                break\n",
        "\n",
        "        # If still empty, relax filters for this item\n",
        "        if len(kept) == 0:\n",
        "            # allow same-category and lower score to ensure at least 1 suggestion\n",
        "            nbrs_relaxed = topk_neighbors_embed(pid, k=10) or topk_neighbors_fallback(pid, k=10)\n",
        "            if nbrs_relaxed:\n",
        "                nid, score = nbrs_relaxed[0]\n",
        "                kept = [(nid, score)]\n",
        "\n",
        "        for nid, score in kept:\n",
        "            rows.append({\n",
        "                \"item_A_id\": pid,\n",
        "                \"item_A_name\": nice_name(pid),\n",
        "                \"cat_A\": category(pid),\n",
        "                \"item_B_id\": nid,\n",
        "                \"item_B_name\": nice_name(nid),\n",
        "                \"cat_B\": category(nid),\n",
        "                \"affinity_score\": round(float(score), 4),\n",
        "                \"source\": \"embed\" if nid in vocab_index and pid in vocab_index else \"fallback_jaccard\"\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "per_item = recommend_for_all(\n",
        "    k_per_item=5,\n",
        "    min_score=0.01,        # lower if coverage still thin\n",
        "    cross_category=False,   # set False if you also want similar/substitutes\n",
        "    prefer_embed=True\n",
        ")\n",
        "\n",
        "# This file guarantees each product has up to K suggestions.\n",
        "per_item.to_csv(\"marketing_pairs_per_item_3.csv\", index=False)\n",
        "\n",
        "# If you still want a deduped global pair list (A,B unique, max score):\n",
        "pairs = (per_item\n",
        "    .assign(key=lambda d: d.apply(lambda r: tuple(sorted([r[\"item_A_id\"], r[\"item_B_id\"]])), axis=1))\n",
        "    .sort_values(\"affinity_score\", ascending=False)\n",
        "    .drop_duplicates(\"key\")\n",
        "    .drop(columns=[\"key\"])\n",
        ")\n",
        "pairs.to_csv(\"marketing_pairs_top_3.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yhmFTNekk5dH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Categories: Fresh Produce Fresh Produce\n",
            "Jaccard: 0.04933979742906151\n",
            "In vocab: True True\n",
            "Neighbors of onion (embed): [('501', 0.14396953582763672), ('502', 0.12163427472114563), ('303', 0.11718130111694336), ('603', 0.11105742305517197), ('106', 0.10972393304109573)]\n",
            "Neighbors of garlic (embed): [('304', 0.3299647867679596), ('703', 0.3055746257305145), ('103', 0.17175957560539246), ('505', 0.14696305990219116), ('205', 0.137779101729393)]\n",
            "Neighbors of onion (jaccard): [('501', 0.4575766253808399), ('502', 0.45754745141428893), ('704', 0.4564322442718971), ('106', 0.45631380337636546), ('302', 0.3438990086103393)]\n",
            "Neighbors of garlic (jaccard): [('801', 0.45790694067159576), ('101', 0.4577607049691417), ('301', 0.34467968372715035), ('401', 0.3441625901169594), ('204', 0.2809024141342188)]\n"
          ]
        }
      ],
      "source": [
        "# Replace with the actual string IDs used in `baskets`/df\n",
        "onion = '105'     # e.g., Yellow Onion id\n",
        "garlic = '109'    # e.g., Garlic Head id\n",
        "\n",
        "print(\"Categories:\", category(onion), category(garlic))\n",
        "\n",
        "# 1) Do they co-occur at all?\n",
        "print(\"Jaccard:\", jaccard(onion, garlic))\n",
        "\n",
        "# 2) Are both in embedding vocab?\n",
        "print(\"In vocab:\", onion in vocab_index, garlic in vocab_index)\n",
        "\n",
        "# 3) What do embeddings say (raw top-k without your business filters)?\n",
        "print(\"Neighbors of onion (embed):\", topk_neighbors_embed(onion, k=20)[:5] if onion in vocab_index else \"N/A\")\n",
        "print(\"Neighbors of garlic (embed):\", topk_neighbors_embed(garlic, k=20)[:5] if garlic in vocab_index else \"N/A\")\n",
        "\n",
        "# 4) What does fallback Jaccard say (raw top-k)?\n",
        "print(\"Neighbors of onion (jaccard):\", topk_neighbors_fallback(onion, k=20)[:5])\n",
        "print(\"Neighbors of garlic (jaccard):\", topk_neighbors_fallback(garlic, k=20)[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
